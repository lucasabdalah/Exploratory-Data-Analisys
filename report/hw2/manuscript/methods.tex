\section{Methods}

\subsection{Data Overview}

\subsection{Data Preprocessing}
\reviewUrgent{Z-Score Normalization}

\subsection{Cross Validation}
A validação cruzada consiste numa técnica para avaliar o modelo dentro conjunto de teste, geralmente levando em conta a flexibilidade do modelo e o erro quadrático médio. Em suma divide-se o conjunto em k grupos distintos de tamanhos semelhantes, esses grupos um é removido e passa ser o conjunto de validação, então se produz os modelos a partir das amostras restantes, e para verificar  eu funcionamento tenta-se prever o conjunto de validação. Então o grupo removido retorna ao conjunto de treino e o grupo seguinte é removido e se torna o conjunto de validação. Esse processo é repetido até que todos os grupos sejam utilizados como validadores.

Essa estratégia serve muitas vezes para indicar quais modelos terão uma previsão melhor no conjunto de teste, visto que permite comparar os níveis de erro e variância gerado pelos modelos. Importante ressaltar que usar k muito pequeno (como apenas em dois grupos k = 2) ou muito grande (k = número de amostras) gerará problemas. No primeiro caso poderá um alto enviesamento dos modelos, visto que muitas amostras serão deixadas de fora do grupo de treino, e no segundo ocorrerá muita variância nos modelos pois os grupos utilizados no método são muito semelhantes. Portanto para compensar ambos os efeitos costuma-se usar k = 5 ou 10, visto que experimentalmente apresentam níveis aceitáveis de variância e enviesamento.

\subsection{Linear Regression}
Os modelos de regressão linear são um conjunto de técnicas de análise estatística de dados, as quais tem por objetivo prever uma tendência de comportamento de um dado, a partir de outros dados já obtidos, tendo como pré-suposto que cada um dos dados já conhecidos, os preditores, tem uma relação linear com o dado de saída.

Portanto esses modelos são definidos por seguirem a
seguinte equação:
\begin{equation}
  y_i = \beta_0 + \sum_{i=1}^n(\beta_i x_i + e_i)
\end{equation}
Sendo yi o valor de cada saída, xi cada preditor, $\beta_i$ os
coeficientes que determinam a relação de cada um dos
preditores com a saída, e ei os erros que não podem ser
previstos, ou seja, o típico ruído.
Os parâmetros $\beta_i$ não podem ser definidos com abso-
luta certeza, em virtude dos elementos de erro ei. Entre-
tanto, sendo o erro suficientemente pequeno, é possível
obter uma boa aproximação desses valores. O objetivo
se torna definir o plano que minimiza a soma dos er-
ros quadráticos entre os valores reais e os estimados, a
ponto que a diferença seria apenas gerada pelo fator de
erro ei.
% SSE = n∑ i=1 (yi − ˆyi)2 (2)
Sendo SSE a soma dos erros quadráticos e yi o valor de
cada predição do modelo. Assim, podemos obter esti-
mativas coerentes a partir da seguinte equação vetorial:
$\beta$ = (XT X) - 1 XT y (3)
Onde X = (X1, X2, . . . , Xn), y o vetor com cada res-
posta, e $\beta$ o vetor com a estimação dos parâmetros $\beta_i$.
Métodos desse tipo são aplicáveis em diversos tipos de
previsões, desde observações químicas a efeitos econômi-
cos. Isso se deve ao fato de ser facilmente compreensível
e de fácil aplicação. Porém, caso haja uma relação não
linear relevante entre os preditores e a saída, esses mode-
los não serão capazes de prever corretamente a variável
desejada.


\subsection{Penalized Models}
Os modelos de regressão simples costumam não ter enviesamento, porém podem podem apresentar um certo nível de variância em relação aos resultados finais com suas previsões. Então, para tal visa-se aumentar levemente o enviesamento dos dados a fim de obter um decrescimento mais expressivo na variância do modelo. Para isso altera-se a equação de minimização da somas dos erros quadráticos adicionando um novo elemento: 
% SSEL2 = n∑ i=1 (yi − ˆyi)2 + λ n∑ i=1 β2 i (4)
Sendo $\lambda$ um parâmetro a ser determinado experimentalmente. A equação 4 serve para controlar as grandezas dos parâmetros  $\beta_i$, permitindo que seus valores sejam altos apenas se contribuírem para a minimização da equação. Então a medida que $\lambda$ aumenta os fatores  $\beta_i$ tendem a diminuir, podendo muitas vezes tornar o preditor associado desprezível. Por isso esse método faz parte do conjunto de métodos denominados métodos de diminuição. Portanto sendo feito os ajustes do parâmetro $\lambda$ pode-se chegar a um modelo com variância menor e com baixo enviesamento, o que pode tornar o modelo mais atrativo que a regressão simples.

\subsection{Principal Component Regression}
Como o conjunto de dados utilizado possui um grande
número de preditores, seria interessante reduzir esse número para tornar o modelo mais simples e menos custoso computacionalmente. Para isso, uma das estratégias é achar as chamadas componentes principais, que são definidas pelos autovetores da matriz de covariância dos preditores. Assim projeta-se os dados em um número reduzido de preditores, aquelas atreladas aos maiores autovalores, ou seja as que apresentam uma variabilidade maior. Existem dois problemas com esse método, o primeiro é que se torna difícil a interpretação das componentes, o segundo é que o método não define as componentes pela sua relação com a saída, o que pode fazer que as componentes dominantes não apresentem correlação com a saída, o que prejudica o desenvolvimento de um modelo eficiente, pois não se pode ter controle sobre a relação dos novos preditores com a saída.

\subsection{Partial Least Squares}
O modelo de regressão de mínimos parciais pode ser visto como uma junção das funcionalidades dos modelos de regressão linear, que buscam maximar a correlação dos preditores com saída, e os em componentes principais, que capturam as maiores variâncias nos preditores. Assim, é um método supervisionado que gera novas componentes que tenham máxima covariância com a saída, assim permitindo um número menor de componentes necessárias em relação ao PCR. Entretanto o problema da interpretabilidade dos novos preditores ainda persiste. Existem alguns algorítimos para o calculo do PLS, como o NIPALS e o SIMPLS.